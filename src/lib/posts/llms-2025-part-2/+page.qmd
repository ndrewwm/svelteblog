---
title: "How are we feeling about LLMs in 2025? (part 2)"
author: Andrew Moore
date: "2025-09-01"
slug: llms-2025-part-2
description: |
  Part 2 of my midyear temperature check on LLMs. An aside on how useful these tools are for software development, some thoughts on model collapse, and my impressions of where things stand re: the environmental impact of LLM usage.
draft: false
categories: ["temp-check", "LLMs"]
preview: null
knitr:
  opts_knit: 
    base.url: "/"
  opts_chunk:
    fig.path: "./+page_files/"
    class-output: qmdresults
    class-message: qmdmessage
    warning: false
    message: false
    fig-align: center
format:
  gfm:
    variant: +yaml_metadata_block
---

<svelte:head>
  <script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>
</svelte:head>


This is a continuation of my midyear temperature check regarding LLMs; you can find part 1 [here](/llms-2025). In this post, I'm gathering my thoughts about model collapse, and the environmental impacts of the tech (with a general focus on how this is playing out in the United States, but I might branch out-- we'll see). But, first a quick return to the topic of LLMs being used to support developer productivity.

### Aside: productivity boosts from LLM-based tools are not achieved as easily as anecdotes might suggest.

I know I've already talked about LLMs and their usage related to software development at length, but the day after posting part 1, I came across a post from METR (Model Evaluation & Threat Research) reporting an RCT they'd conducted. METR recruited 16 experienced developers responsible for maintaining large open-source GitHub repositories (averaging 1M+ lines of code being maintained). The developers identified 246 high-priority issues (bug fixes or feature requests) across the repositories to address. METR then randomly assigned whether a developer was allowed to use LLM-based tools (Claude Cursor, Copilot, etc.) to complete the issue. During and after developers wrote code to address an issue, they were asked to estimate the degree to which the LLM-tool(s) impacted their speed. Here's a key visualization summarizing one of their findings:

<center>
<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:dll3hepzq76nymel5c3yt6nk/app.bsky.feed.post/3ltn3t3amms2x" data-bluesky-cid="bafyreif63ykwtnggvqdmk57fbq4khfpkvykiwngmsnofvs7bv2rikr4aym" data-bluesky-embed-color-mode="system">We ran a randomized controlled trial to see how much AI coding tools speed up experienced open-source developers.

The results surprised us: Developers thought they were 20% faster with AI tools, but they were actually 19% slower when they had access to AI than when they didn't.

<a href="https://bsky.app/profile/did:plc:dll3hepzq76nymel5c3yt6nk/post/3ltn3t3amms2x?ref_src=embed">[image or embed]</a>&mdash; METR (<a href="https://bsky.app/profile/did:plc:dll3hepzq76nymel5c3yt6nk?ref_src=embed">@metr.org</a>) <a href="https://bsky.app/profile/did:plc:dll3hepzq76nymel5c3yt6nk/post/3ltn3t3amms2x?ref_src=embed">July 10, 2025 at 1:47 PM</a></blockquote>
</center>

In summary, all the parties involved in this study (including the authors) _overestimated_ the degree to which LLM-tools would boost productivity. Instead, their results show that using LLMs actually decreased their productivity. One study doesn't close discussion, but this feels like reasonable empirical support to the notions that LLMs aren't very helpful for maintaining/extending existing projects, and that _self-reports aren't reliable._ The benefits from these tools aren't accrued uniformly, as I mentioned in part 1. There's a useful idea from computer science known as ["Amdahl's Law"](https://en.wikipedia.org/wiki/Amdahl%27s_law), which states that performance improvements gained through optimizing a single part of a system are limited by the proportion of time that part is actually used. LLMs may allow us to throw spaghetti (code) at the wall at unprecedented rates, but this likely isn't the most important bottleneck (further, this ability can be [actively harmful](https://www.theregister.com/2025/07/21/replit_saastr_vibe_coding_incident/)).

### Adjusting: we're still waiting for (model) collapse.

In part 1, I talked about my fears that LLM-based tools like ChatGPT would negatively impact knowledge-sharing communities like StackOverflow. However, I didn't discuss an important aspect of this recent history, which is that the site's moderators [banned the use of ChatGPT](https://meta.stackoverflow.com/questions/421831/policy-generative-ai-e-g-chatgpt-is-banned) for generating questions/answers shortly after its release. Not only are LLMs capable of spitting out Javascript at inhuman speeds, they're able to do the same for any written text. This poses an ecological problem for the broader internet, where a large share of communication is still text-based. The rate at which spam can be produced has always been a strength in the long-running conflict between spammers and mods. It's pretty bad that LLMs are augmenting this already considerable advantage.

The issue is that human comprehension is always slower than the rate of text-production. For a Q&A site like StackOverflow, random users can collectively "answer" thousands of questions via LLM-tools in an afternoon, but the site's human users still need to determine if any of this content is actually useful or applicable. The fluency and dynamic nature of LLM outputs means that certain anti-spam automations are less effective.[^1] As we've seen over the past few years, LLMs are being used to generate email replies, social media posts, long-form documentation, essays, reports, articles, novels, etc. Various forms of slop, as the kids say. Much of this LLM output is intended to land somewhere on the internet, where many people can read it. This being the case, it seems inevitable that future generations of LLMs will ingest LLM-generated text as part of their training.

A reasonable intuition is that this seems bad for the future utility of these models. LLMs model natural language probabilistically; as machines, they're products of statistical learning upon massive corpora of textual data. Without getting into the weeds, you could say that LLMs are approximating a probability distribution of (textual) language.[^2] This being the case, LLMs share a quality of all statistical models, in that their performance depends on the quality and diversity of data used for training. "Garbage in, garbage out" (GIGO), as the saying goes. To the point: indiscriminantly mixing _generated_ text with human-written text for training seems questionable. So, what will happen to these models as output from prior LLMs accumulates in their training data? I was excited to find this paper in 2024, [_"AI models collapse when trained on recursively generated data"_](https://www.nature.com/articles/s41586-024-07566-y) (Shumailov et al., 2024), which works through some of the theory behind this question.

#### What is model collapse?

<!-- 2023: https://www.hendrik-erz.de/post/why-gzip-just-beat-a-large-language-model -->
<!-- 2024: Language Modeling is Compression https://arxiv.org/pdf/2309.10668 -->

<!-- sources of error:
- statistical approximation error
- functional expressivity error
- functional approximation error -->

As the paper's authors describe it, _model collapse_ is "a degenerative process whereby, over time, models forget the true underlying data distribution, even in the absence of a shift in the distribution". The phenomena is caused by "indiscriminate learning" from data produced by other models, rather than actual examples (i.e., for LLMs, human-written language). As we've seen, LLM providers train their models generationally, with each new generation getting access to newer data. Shumailov et al. speculate about what might happen in a future where "_GPT-n_" is trained on data consisting of mostly LLM output. Their conclusions, based on theoretical results and experimentation, motivate the authors' concerns that use of LLMs at scale to publish internet content will contaminate the data necessary for training successor models.

The next few paragraphs are a little technical, but I want to walk through some of what the authors are discussing. In particular, I want to reproduce one of their simpler examples to help demonstrate what the authors mean by a model "forgetting" part of the original distribution. This involves a bit of mathematical notation, but I hope you'll bear with me!

The authors describe a generational process of model fitting, in which model outputs are subsequently used for training future generations. This process is model-agnostic, i.e., it applies to the training of LLMs, and other categories of predictive models. In the authors' notation, they describe a dataset, $\mathcal{D}_i$, comprised of random variables from generation $i$:

$$
\mathcal{D}_i = \{ X^i_1, X^i_2, \cdots, X^i_j, \cdots, X^i_{M_i} \overset{i.i.d.}{\sim} p_i(\theta) \}. 
$$

A _functional approximation_ of $\mathcal{D}_i$'s probability distribution is estimated via training: $p_{\theta_{i+ 1}} = \mathcal{F}_\theta (p_i)$. The next generation's dataset, $\mathcal{D}_{i + 1}$, is generated by sampling from

$$
\begin{align*}
p_{i + 1} = \alpha_i p_{\theta_{i + 1}} + \beta_i p_i + \gamma_i p_0, \text{ where } &\alpha_i, \beta_i, \gamma_i > 0 \\
\text{ and } &\alpha_i + \beta_i + \gamma_i = 1.
\end{align*}
$$

Here, $\alpha_i$, $\beta_i$, and $\gamma_i$ represent proportions of $\mathcal{D}_{i + 1}$ coming from generation $i$, $i - 1$, and the original data, respectively.[^3] To demonstrate model collapse, they discuss several examples, setting $\alpha_i = 1$ and $\beta_i = \gamma_i = 0$.

As is tradition, the authors provide an example using the 1-dimensional Gaussian distribution to build intuition. We can treat this as a vastly simpler stand-in for the more involved process of building an LLM with billions of parameters. To model a Gaussian distribution (the classical "bell curve"), we only need to estimate two parameters from a dataset $\mathcal{D}_i$: the mean $\mu_{i + 1}$, and variance $\sigma^2_{i + 1}$. Once these parameters are found, we can sample from $p_{i + 1}$ to produce $\mathcal{D}_{i + 1}$: 

$$
\begin{align*}
\mathcal{D}_i &= \{ X^i_1, \cdots, X^i_{M_i} \overset{i.i.d.}{\sim} \mathcal{N}(\mu_i, \sigma^2_i) \} \\
\mu_{i + 1} &= \frac{1}{M_i} \sum_j X^i_j \\ \sigma^2_{i + 1} &= \frac{1}{M_i - 1} \sum_j (X^i_j - \mu_{i + 1})^2 \\
\mathcal{D}_{i + 1} &= \{ X^{i + 1}_1, \cdots, X^{i + 1}_{M_{i + 1}} \overset{i.i.d.}{\sim} \mathcal{N}(\mu_{i + 1}, \sigma^2_{i + 1}) \}
\end{align*}
$$

Visualizing this process via simulation (using initial values $\mu_0 = 0$, and $\sigma^2_0 = 1$), we clearly see how the distribution's variance (the distribution's "spread") approaches 0 in just a few generations. You could say that after the first generation, the model "forgets" about the possibility of data with an absolute value greater than 2. By the fifth generation, all values are between -0.19 and 0.16.

```{r}
#| echo: false
#| message: false
#| warning: false

source("../../assets/theme-ndrewwm.r")
theme_set(theme_ndrewwm() + theme(panel.grid.minor = element_blank()))
set.seed(202508)

sim <- function(m, D, mu_0, sd_0, alpha, beta, gamma) {
  mu <- mu_0
  sd <- sd_0

  d <- tibble()
  for (i in 1:D) {
    mu_1 <- mu
    sd_1 <- sd
    x <- c(
      rnorm(m * alpha, mu, sd),
      rnorm(m * beta, mu_1, sd_1),
      rnorm(m * gamma, mu_0, sd_0)
    )

    mu <- mean(x)
    sd <- (m - 1)^(-1) * sum((x - mu)^2)
    d <- bind_rows(d, tibble(i = i, x = x))
  }

  return(d)
}

D <- 6
d <- sim(m = 100, D = D, mu_0 = 0, sd_0 = 1, alpha = 1, beta = 0, gamma = 0) |>
  mutate(
    i = factor(
      i,
      labels = c("i = 1", 2:D),
      levels = 1:D
    )
  )

ggplot(d, aes(x)) +
  geom_histogram() +
  facet_wrap(~i, nrow = 1) +
  labs(x = "x", y = "# samples")
```

This toy example risks being so simple as to be obvious, but it illustrates how building models recursively can be fraught. More interestingly, however, the authors demonstrate that LLMs are vulnerable to the same problem. By fine-tuning a published model over successive generations (in which the model is trained with samples from the prior generation), high probability outputs from generation 0 become even more probable in the model's future descendants. Here's a passage from the paper, which you may have already seen in popular coverage:

> Example of text outputs of an OPT-125m model affected by model collapse—models degrade over generations, for which each new generation is trained on data produced by the previous generation.
> 
> **Input:** some started before 1360 — was typically accomplished by a master mason and a small team of itinerant masons, supplemented by local parish labourers, according to Poyntz Wright. But other authors reject this model, suggesting instead that leading architects designed the parish church towers based on early examples of Perpendicular.
> 
> **Outputs:**
> 
> **Gen 0:** Revival architecture such as St. John’s Cathedral in London. The earliest surviving example of Perpendicular Revival architecture is found in the 18th @-@ century Church of Our Lady of Guernsey, which dates from the late 19th century. There are two types of perpendicular churches : those.
> 
> **Gen 1:** architecture such as St. Peter’s Basilica in Rome or St. Peter’s Basilica in Buenos Aires. There is no evidence that any of these buildings were built during the reign of Pope Innocent III, but it is possible that they may have been built during the reign of his successor, Pope Innocent.
> 
> **Gen 5:** ism, which had been translated into more than 100 languages including English, French, German, Italian, Spanish, Portuguese, Dutch, Swedish, Norwegian, Polish, Hungarian, Slovak, Lithuanian, Estonian, Finnish, Romanian, Bulgarian, Turkish, Croatian, Serbian, Ukrainian, Russian, Kazakh, Kyrgyz.
> 
> **Gen 9:** architecture. In addition to being home to some of the world’s largest populations of black @-@ tailed jackrabbits, white @-@ tailed jackrabbits, blue @-@ tailed jackrabbits, red @-@ tailed jackrabbits, yellow @-.

"Collapse" certainly feels apt as a description for what we see as a response in Gen 9. That said, if we're persuaded by the authors' results, how should we think about what they suggest for models trained in the (near) future? The authors consider two different scenarios for training LLMs: $\gamma_i = 0.0$, and $\gamma_i = 0.1$. The first imagines a world in which all text that's available for training is output from a prior LLM. We can grant that this isn't meant to be realistic, and understand it's meant to demonstrate the process of interest. But, the second scenario, $\gamma_i = 0.1$, also doesn't feel satisfying to me; it reads too similar to the first. The authors show that model collapse still occurs, just a bit more slowly. In their supplemental materials, they also observe that the issue persists in the long-run when you randomly vary $\alpha_i$, $\beta_i$, and $\gamma_i$. Fair enough, but the most realistic scenario seems like one where we see practitioners curating $\mathcal{D}_{i + 1}$ by keeping $\gamma_i$ as large as possible, for as long as possible.

- it hasn't happened (yet)
- metaphor of pure steel (uncontaminated from nuclear radiation)
- returning to the topic of search: retrieval augmented generation (RAG)
  - adding the contents of the web to the the LLM's context window
  - slop serving slop

### Adjusting: concerns related to the environmental impact of LLMs may have been overstated.

- LLMs take _more_ energy than standard searches, but it's not drastically different?
  - Need to take this with a grain of salt, given that providers (like Google) are the ones that produce the estimates, but it's what I'm aware of?

- Data centers use a lot of water and depending on fuel source maybe more or less harmful, but
  - Agricultural practices and transit still the most problematic for US society, wrt reducing emissions, I think?

- The sociotechnical impacts of the technology seem like the chief area of concern; we need to address them head on, and ensure our critiques are effective (how are they actually harmful, what do they enable, what are they being used to do?)
  - ChatGPT-4 seems to have contributed to suicides; they appear to be causing or exacerbating all sorts of mental health problems!

- Economic fallout from the collapse of the AI bubble looks like it will affect more than just Silicon Valley
  - enormous amount of our GDP appears to be tied up in construction of data centers
  - might be that we can use all this capacity for something else, but... what will that be?

[^1]: In some cases, traditional approaches still work when spam-producers don't bother to remove common tells in chatbot output (like "happy to help!", or "certainly!"). But, we've definitely kicked off a new arms-race between factions in this conflict. 

    I haven't discussed the issue in either of these posts, but it's extremely concerning to read reports about LLM usage for writing assignments in high-school and college. Automated approaches for plagiarism detection will inevitablely miss some true instances, and injure honest students via false positives. I don't think we were effectively engaging with these nuances as a society before 2022. The fact that this problem is more urgent today is not encouraging.

[^2]: I think this description is workable, but I feel unsatisfied in stating it. If you treat prediction as a problem to be solved, probability theory is certainly applicable to the task of predicting future tokens. But, even if we grant that LLM outputs qualify as "language", I don't know if the probability distribution they model can faithfully describe "our" language(s). 

    Training the foundation models required massive amounts of text. Necessarily, this means that text used for training is sourced across large spans of time, and relects what was been _available_ for scraping. Some argue that this resembles how humans acquire their own linguistic abilities (i.e., people develop their way of writing/speaking via cumulative experiences and sampling), but I think this analogy is unconvincing. Children don't read the entirety of Wikipedia to acquire fluency; these are vastly different processes, and their comparison is largely unhelpful.

    I think a moderate perspective is that LLMs produce "machine language". Their outputs resemble what humans produce, but the qualifier is a necessary floor for discussion. We should be foregrounding this difference (rather than eliding it).

[^3]: The definition for $p_{i + 1}$ and $\alpha$, $\beta$, and $\gamma$ characterizes $p_{i + 1}$ as a [finite mixture distribution.](https://en.wikipedia.org/wiki/Mixture_distribution)

<style>
  h3 {
    font-size: 1.25em;
    margin-bottom: .8em;
  }
  h4 {
    font-size: 1.15em;
    margin-bottom: .8em;
  }
  img {
    object-fit: scale-down;
    max-width: 100vw;
  }
</style>
